{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48f13e8",
   "metadata": {},
   "source": [
    "# Kafka + SGP4 Pipeline Interactive Testing\n",
    "\n",
    "This notebook tests the complete pipeline:\n",
    "**TLE API → Kafka → SGP4 Computation → HDFS Storage**\n",
    "\n",
    "Sections:\n",
    "1. Verify Kafka connection and consume TLE messages\n",
    "2. Parse TLE data and compute SGP4 vectors locally\n",
    "3. Test Spark SGP4 pipeline\n",
    "4. Query HDFS output (Parquet files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c45f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from kafka import KafkaConsumer\n",
    "from sgp4.api import Satrec, jday\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39969f53",
   "metadata": {},
   "source": [
    "## 1. Connect to Kafka and Read TLE Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a58994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'space_debris_tle',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=True,\n",
    "    group_id='notebook_test_group',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print(\"✓ Connected to Kafka broker at localhost:9092\")\n",
    "print(f\"✓ Subscribed to topic: space_debris_tle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca522862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 5 sample messages\n",
    "messages = []\n",
    "for i, message in enumerate(consumer):\n",
    "    if i >= 5:\n",
    "        break\n",
    "    messages.append(message.value)\n",
    "    \n",
    "print(f\"Read {len(messages)} TLE messages from Kafka\\n\")\n",
    "print(\"Sample message:\")\n",
    "print(json.dumps(messages[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a8f0d",
   "metadata": {},
   "source": [
    "## 2. Parse TLE and Compute SGP4 Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7b0d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sgp4_vectors(tle_data):\n",
    "    \"\"\"Compute SGP4 position and velocity vectors from TLE data\"\"\"\n",
    "    try:\n",
    "        # Initialize SGP4 satellite\n",
    "        satellite = Satrec.twoline2rv(\n",
    "            tle_data['tle_line1'],\n",
    "            tle_data['tle_line2']\n",
    "        )\n",
    "        \n",
    "        # Parse epoch from TLE\n",
    "        epoch_str = tle_data.get('epoch', tle_data.get('EPOCH'))\n",
    "        epoch_dt = datetime.fromisoformat(epoch_str.replace('Z', '+00:00'))\n",
    "        \n",
    "        # Convert to Julian Date\n",
    "        jd, fr = jday(epoch_dt.year, epoch_dt.month, epoch_dt.day,\n",
    "                      epoch_dt.hour, epoch_dt.minute, epoch_dt.second)\n",
    "        \n",
    "        # Propagate to epoch time\n",
    "        error_code, position, velocity = satellite.sgp4(jd, fr)\n",
    "        \n",
    "        if error_code != 0:\n",
    "            return None, f\"SGP4 error: {error_code}\"\n",
    "        \n",
    "        # Calculate altitude and velocity magnitude\n",
    "        pos_magnitude = np.linalg.norm(position)\n",
    "        vel_magnitude = np.linalg.norm(velocity)\n",
    "        altitude_km = pos_magnitude - 6371.0  # Earth radius\n",
    "        \n",
    "        return {\n",
    "            'satellite_id': tle_data.get('satellite_id', tle_data.get('NORAD_CAT_ID')),\n",
    "            'epoch_time': epoch_str,\n",
    "            'position_x': position[0],\n",
    "            'position_y': position[1],\n",
    "            'position_z': position[2],\n",
    "            'velocity_x': velocity[0],\n",
    "            'velocity_y': velocity[1],\n",
    "            'velocity_z': velocity[2],\n",
    "            'altitude_km': altitude_km,\n",
    "            'velocity_magnitude_kms': vel_magnitude,\n",
    "            'inclination': tle_data.get('inclination', tle_data.get('INCLINATION')),\n",
    "            'eccentricity': tle_data.get('eccentricity', tle_data.get('ECCENTRICITY')),\n",
    "            'mean_motion': tle_data.get('mean_motion', tle_data.get('MEAN_MOTION'))\n",
    "        }, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87643c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SGP4 vectors for sample messages\n",
    "sgp4_results = []\n",
    "\n",
    "for msg in messages:\n",
    "    result, error = compute_sgp4_vectors(msg)\n",
    "    if result:\n",
    "        sgp4_results.append(result)\n",
    "    else:\n",
    "        print(f\"Error for satellite {msg.get('satellite_id')}: {error}\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_sgp4 = pd.DataFrame(sgp4_results)\n",
    "print(f\"\\nComputed SGP4 vectors for {len(df_sgp4)} satellites\\n\")\n",
    "print(df_sgp4[['satellite_id', 'altitude_km', 'velocity_magnitude_kms', 'inclination']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd57d47",
   "metadata": {},
   "source": [
    "## 3. Visualize Position Vectors (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a190626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot satellite positions\n",
    "ax.scatter(df_sgp4['position_x'], df_sgp4['position_y'], df_sgp4['position_z'], \n",
    "           c='red', marker='o', s=100, label='Satellites')\n",
    "\n",
    "# Plot Earth (sphere)\n",
    "u = np.linspace(0, 2 * np.pi, 50)\n",
    "v = np.linspace(0, np.pi, 50)\n",
    "earth_radius = 6371.0\n",
    "x = earth_radius * np.outer(np.cos(u), np.sin(v))\n",
    "y = earth_radius * np.outer(np.sin(u), np.sin(v))\n",
    "z = earth_radius * np.outer(np.ones(np.size(u)), np.cos(v))\n",
    "ax.plot_surface(x, y, z, color='blue', alpha=0.3)\n",
    "\n",
    "ax.set_xlabel('X (km)')\n",
    "ax.set_ylabel('Y (km)')\n",
    "ax.set_zlabel('Z (km)')\n",
    "ax.set_title('Satellite Positions in Earth-Centered Inertial (ECI) Frame')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc104a8",
   "metadata": {},
   "source": [
    "## 4. Test Full Spark SGP4 Pipeline\n",
    "\n",
    "The full pipeline runs as a Spark Structured Streaming job. To start it:\n",
    "\n",
    "```bash\n",
    "# Start TLE API\n",
    "cd /home/bharath/Documents/BigData/project/data/Space-Debris-Risk-Prediction\n",
    "uv run src/demo/api/tle_stream_api.py &\n",
    "\n",
    "# Stream data to Kafka\n",
    "uv run src/kafka/tle_api_to_kafka_producer.py --limit 100 &\n",
    "\n",
    "# Run Spark SGP4 job (in Docker)\n",
    "docker exec -u root spark-master /opt/spark/bin/spark-submit \\\n",
    "  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \\\n",
    "  /opt/spark/work-dir/src/kafka/spark_sgp4_to_hdfs.py \\\n",
    "  --kafka broker:29092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f8ea8",
   "metadata": {},
   "source": [
    "## 5. Query HDFS Output (after Spark job runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb0e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell requires PySpark - uncomment after Spark job writes data\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query_SGP4_Vectors\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from HDFS\n",
    "df_hdfs = spark.read.parquet(\"/tmp/sgp4_vectors/\")\n",
    "\n",
    "# Show sample data\n",
    "df_hdfs.select('satellite_id', 'altitude_km', 'velocity_magnitude_kms', 'epoch_time').show(10)\n",
    "\n",
    "# Count total records\n",
    "print(f\"Total SGP4 vectors in HDFS: {df_hdfs.count()}\")\n",
    "\n",
    "# Query satellites above 400km altitude\n",
    "high_altitude = df_hdfs.filter(df_hdfs.altitude_km > 400)\n",
    "print(f\"Satellites above 400km: {high_altitude.count()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612dc2b",
   "metadata": {},
   "source": [
    "## 6. Verify Current Kafka Topic Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2afc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "\n",
    "# Connect to Kafka admin\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    client_id='notebook_admin'\n",
    ")\n",
    "\n",
    "# List topics\n",
    "topics = admin_client.list_topics()\n",
    "print(\"Available Kafka topics:\")\n",
    "for topic in topics:\n",
    "    print(f\"  - {topic}\")\n",
    "\n",
    "# Get consumer group offsets (requires consumer to have committed)\n",
    "consumer_groups = admin_client.list_consumer_groups()\n",
    "print(f\"\\nActive consumer groups: {len(consumer_groups)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
